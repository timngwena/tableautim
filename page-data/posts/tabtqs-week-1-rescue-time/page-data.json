{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/tabtqs-week-1-rescue-time/","result":{"data":{"markdownRemark":{"id":"a67cdf71-c852-555e-8032-92bee9dd8525","html":"<p><img src=\"/media//2017/03/Alltime.png\"> </p>\n<p>Leading up to <a href=\"https://www.meetup.com/data-visual-London/events/238037230/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">several talks I’ll be giving in April</a>, I’m going to do a series of mini posts highlighting the process behind the quantified self workflows that I use. These will typically include Tableau and Alteryx so think of this as a how to guide geared towards these tools or more a workflow to replicate in your tool of choice. This week its Rescue Time. This service sits on your machine and tracks everything you do. If you install the browser extension, it tracks every website you visit. They then do some crowdsourced categorisation of your activities to inform your productivity. Net result is something like this…</p>\n<p><img src=\"/media//2017/03/year-stats.png\"> </p>\n<p>Of course the way these tools represent the data never really hits the spot and I had several questions I wanted to answer specifically around the number of hours I’ve spent using Tableau, caveat being, rescue time only captured usage on computers it was installed in and try as I may, I couldn’t quite convince the IT guys at some of the clients I had worked at that this was a great tool. Rescue time have an API, I checked it out and all it needs are some text fields cleverly inserted into a url with a format stuck on the end so I fired up Alteryx and this is what the API url looks like. </p>\n<p><img src=\"/media//2017/03/key.png\"> </p>\n<p>The full workflow looks like this.</p>\n<p><img src=\"/media//2017/03/workflow.png\"> </p>\n<p>A few details about the workflow, It was built in 20 minutes so likely could be optimised, however the key sections are the rate throttler and the json parsing. I had initially requested  CSVs but I couldnt quite figure out how to handle the paged response so i opted for the easy way out building a json parsing section to parse the json response. This pulled out 283,000 rows of data. The rate throttler is there to avoid annoying the Rescue Time server.Without it, Alteryx would ping the requests to rescue time as quick as it could and to some servers that could look like the start of a DOS attack or I might simply hit the rate limit on the API and get a bunch of failed responses rather than data. From here the file goes to Tableau, where I can start to create some calculations and charts. Below is the output. </p>\n<p><img src=\"/media//2017/03/Alltime.png\"> </p>\n<p><img src=\"/media//2017/03/Weekly.png\"> </p>","fields":{"slug":"/posts/tabtqs-week-1-rescue-time/","tagSlugs":["/tag/alteryx/","/tag/quantified-self/","/tag/tableau/"]},"frontmatter":{"date":"2017-03-14T22:40:32.169Z","description":"Leading up to several talks I'll be giving in April, I'm going to do a series of mini posts highlighting the process behind the quantified self workflows that I use. These will typically include Tableau and Alteryx so think of this as a how to guide geared towards these tools or more a workflow to replicate in your tool of choice. This week its Rescue Time.","tags":["Alteryx","Quantified Self","Tableau"],"title":"#tabtQS 1: RescueTime in Tableau","socialImage":"/media/gutenberg.jpg"}}},"pageContext":{"slug":"/posts/tabtqs-week-1-rescue-time/"}}}